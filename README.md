 ğŸ˜„ Emojify: Text-to-Emoji Classifier

This repository demonstrates how to build two emoji prediction models:
1. A simple softmax classifier based on word vector averages
2. An LSTM-based deep network using pre-trained GloVe embeddings

---

## ğŸ§  Models Overview

### ğŸ”¸ Softmax Classifier (Model 1)
- Converts sentences to averaged GloVe word vectors
- Uses a NumPy-implemented softmax layer for classification
- Lightweight and interpretable

### ğŸ”¸ LSTM Model (Model 2)
- Builds a Keras model with a pre-trained embedding layer
- Uses LSTM â†’ Dropout â†’ Dense â†’ Softmax pipeline
- Captures contextual dependencies between words

---

## ğŸ§¾ Dataset and Embeddings
- Training: `train_emoji.csv`
- Testing: `tesss.csv`
- Word Vectors: `glove.6B.50d.txt`
- Five emoji classes:
  - â¤ï¸ Love
  - âš½ Sports
  - ğŸ˜„ Happy
  - ğŸ˜ Sad
  - ğŸ´ Food

---

## ğŸ“¦ Key Components

### ğŸ§® Data Utilities
- `read_csv()` â€“ Load training and test CSV files
- `convert_to_one_hot()` â€“ One-hot encodes label indices
- `read_glove_vecs()` â€“ Load and process GloVe vectors

### âœï¸ Feature Extraction
- `sentence_to_avg()` â€“ Convert a sentence to averaged word vector
- `sentences_to_indices()` â€“ Convert sentences to index arrays (for LSTM input)

### ğŸ§  Models
- `model()` â€“ NumPy-based softmax classifier using averaged vectors
- `Emojify_V2()` â€“ Keras LSTM model with pre-trained GloVe embeddings

### ğŸ§ª Evaluation
- `predict()` â€“ Predict labels using a trained model
- `print_predictions()` â€“ Display predictions with emojis
- `plot_confusion_matrix()` â€“ Visualize test accuracy

---

## âš™ï¸ File Structure
```bash
ğŸ“ data/
â”œâ”€â”€ train_emoji.csv
â”œâ”€â”€ tesss.csv
â”œâ”€â”€ glove.6B.50d.txt

ğŸ“ utils/
â”œâ”€â”€ emo_utils.py         # Label/emoji mapping, predictions
â”œâ”€â”€ test_utils.py        # Custom tests for validation
```

---

## ğŸ§ª Sample Results

```text
Sentence: I love you â†’ â¤ï¸
Sentence: Lets play football â†’ âš½
Sentence: Food is ready â†’ ğŸ´
Sentence: Not feeling happy â†’ ğŸ˜
```

---

## ğŸ“ˆ Training Example
```python
model = Emojify_V2((maxLen,), word_to_vec_map, word_to_index)
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X_train_indices, Y_train_oh, epochs=50, batch_size=32)
```

---

## ğŸ§  Embedding Layer
- Constructed from GloVe vectors
- Non-trainable to preserve semantic structure
- Input: sequence of word indices
- Output: sentence embeddings used by LSTM

---

## ğŸ“š References
- [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/)


